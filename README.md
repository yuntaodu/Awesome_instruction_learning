# Awesome-instruction-learning

**Multimodal-instruction-learning**

| number| Title   | Conference/journel + year| Code | Keywords |  Benenit for us |
|  --- |----  | ----  | ---- | ---- | ---- |
|6|3D-LLM: Injecting the 3D World into Large Language Models ([paper](https://arxiv.org/pdf/2307.12981.pdf))|arvix 2023.07|[code](https://github.com/UMass-Foundation-Model/3D-LLM)|3D|new setting|
|5|BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs ([paper](https://arxiv.org/pdf/2307.08581.pdf))|arvix 3023.07|[code](https://bubo-gpt.github.io/)|output with position|new setting|
|4|ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning ([paper](https://arxiv.org/pdf/2307.09474.pdf))|arvix 2023.07|[demo](https://chatspot.streamlit.app/)|input with position|new setting|
|3|Kosmos-2: Grounding Multimodal Large Language Models to the World ([paper](https://arxiv.org/pdf/2306.14824.pdf))|arvix 3023.06|[code](https://github.com/microsoft/unilm/tree/master/kosmos-2)|grounding|new setting|
|2|Shikra: Unleashing Multimodal LLMâ€™s Referential Dialogue Magic ([paper](https://arxiv.org/pdf/2306.15195.pdf))|arvix 2023.06|[code](https://github.com/shikras/)|both input and output with position|new setting|
|1|LLaVA: Large Language and Vision Assistant ([paper](https://arxiv.org/pdf/2304.08485.pdf))|arvix 2023.04|[code](https://github.com/haotian-liu/LLaVA)|New dataset, novel method|the pioneering work|


**Language-instruction-learning**
| number| Title   | Conference/journel + year| Code | Keywords |  Benenit for us |
|  --- |----  | ----  | ---- | ---- | ---- |
|3|Self-Instruct: Aligning Language Models with Self-Generated Instructions ([paper](https://arxiv.org/pdf/2212.10560.pdf))|arvix 2023.08||novel method for dataset generation|good idea|
|2|INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models([paper](https://arxiv.org/pdf/2306.04757.pdf))|arvix 2023.06||||
|1|Scaling Instruction-Finetuned Language Models||||

