# Awesome-multimodal-instruction-learning


| number| Title   | Conference/journel + year| Code | Keywords |  Benenit for us |
|  --- |----  | ----  | ---- | ---- | ---- |
|1|Self-Instruct: Aligning Language Models with Self-Generated Instructions([paper](https://arxiv.org/pdf/2212.10560.pdf))|arvix 2023.08||novel method for dataset generation|good idea|
|2|LLaVA: Large Language and Vision Assistant([paper](https://arxiv.org/pdf/2304.08485.pdf))|arvix 2023.04|[code](https://github.com/haotian-liu/LLaVA)|New dataset, novel method|the pioneering work|
|3|BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs([paper](https://arxiv.org/pdf/2307.08581.pdf))|arvix 3023.07|[code](https://bubo-gpt.github.io/)|output with position|new setting|
|4|ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning([paper](https://arxiv.org/pdf/2307.09474.pdf))|arvix 2023.07|[demo](https://chatspot.streamlit.app/)|input with position|new setting|
|5|Shikra: Unleashing Multimodal LLMâ€™s Referential Dialogue Magic([paper](https://arxiv.org/pdf/2306.15195.pdf))|arvix 2023.06|[code](https://github.com/shikras/)|both input and output with position|new setting|
