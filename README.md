# Awesome-instruction-learning

## Video
| number| Title | speaker| year | Keywords |  quality |
|  --- |----  | ----  | ---- | ---- | ---- |
|1|预训练，指令微调，对齐，专业化：论大语言模型能力的来源([bilibili](https://www.bilibili.com/video/BV1Qs4y1h7pn/?spm_id_from=333.337.search-card.all.click&vd_source=1e56e543df459fd7c33c61150c03f22f))|[yao fu](https://franxyao.github.io/)<br> University of Edinburgh|2023.02|LLM, pre-training, instruction tuning, alignment, specialization|&#9733;&#9733;&#9733;&#9733;&#9733;|

## Multimodal-instruction-learning

| number| Title   | Conference/journel + year| Code | Keywords |  Benenit for us |
|  --- |----  | ----  | ---- | ---- | ---- |
|5|BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs ([paper](https://arxiv.org/pdf/2307.08581.pdf))|arvix 3023.07|[code](https://bubo-gpt.github.io/)|output with position|new setting|
|4|ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning ([paper](https://arxiv.org/pdf/2307.09474.pdf))|arvix 2023.07|[demo](https://chatspot.streamlit.app/)|input with position|new setting|
|3|Kosmos-2: Grounding Multimodal Large Language Models to the World ([paper](https://arxiv.org/pdf/2306.14824.pdf))|arvix 3023.06|[code](https://github.com/microsoft/unilm/tree/master/kosmos-2)|grounding|new setting|
|2|Shikra: Unleashing Multimodal LLM’s Referential Dialogue Magic ([paper](https://arxiv.org/pdf/2306.15195.pdf))|arvix 2023.06|[code](https://github.com/shikras/)|both input and output with position|new setting|
|1|LLaVA: Large Language and Vision Assistant ([paper](https://arxiv.org/pdf/2304.08485.pdf))|arvix 2023.04|[code](https://github.com/haotian-liu/LLaVA)|New dataset, novel method|the pioneering work|


## Language-instruction-learning
| number| Title   | Conference/journel + year| Code | Keywords |  Benenit for us |
|  --- |----  | ----  | ---- | ---- | ---- |
|3|Self-Instruct: Aligning Language Models with Self-Generated Instructions ([paper](https://arxiv.org/pdf/2212.10560.pdf))|arvix 2023.08||novel method for dataset generation|good idea|
|2|INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models([paper](https://arxiv.org/pdf/2306.04757.pdf))|arvix 2023.06||||
|1|Scaling Instruction-Finetuned Language Models||||

